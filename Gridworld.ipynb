{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from enum import IntEnum, Enum"
      ],
      "metadata": {
        "id": "A2BpANMrznNx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Action(Enum):\n",
        "  UP = 'U'\n",
        "  DOWN = 'D'\n",
        "  LEFT = 'L'\n",
        "  RIGHT = 'R'\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.value)"
      ],
      "metadata": {
        "id": "oaOGC8bt14LN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(IntEnum):\n",
        "  ACCESSIBLE_GRID = 0\n",
        "  INACCESSIBLE_GRID = -2\n",
        "  LOSER_GRID = -1\n",
        "  WINNER_GRID = 1"
      ],
      "metadata": {
        "id": "HovS_zDS5LvC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION_SPACE = (Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT)\n",
        "STATE_PROBS = [0.6, 0.35, 0.05] # prob of accessible grid, prob of inaccessible grid, prob of loser grid\n",
        "STATES = [State.ACCESSIBLE_GRID, State.INACCESSIBLE_GRID, State.LOSER_GRID, State.WINNER_GRID]\n",
        "UNKNOWN_POLICY = -2 # the policy is unknown for now, the policies are going to be determined after creating the gridworld\n",
        "ROW_SIZE = 10\n",
        "COLUMN_SIZE = 10\n",
        "THRESHOLD = 1e-3\n",
        "DISCOUNT_FACTOR = 0.9"
      ],
      "metadata": {
        "id": "i3egGS_kzpis"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Gridworld: # Environment\n",
        "  def __init__(self, rows, cols, start):\n",
        "    self.rows = rows\n",
        "    self.cols = cols\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_rewards(self, rewards):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    self.rewards = rewards\n",
        "\n",
        "  def set_actions(self, actions):\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self, s):\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def is_terminal(self, state):\n",
        "    return state not in self.actions\n",
        "\n",
        "  def reset(self):\n",
        "    # put agent back in start position\n",
        "    self.i = ROW_SIZE - 1\n",
        "    self.j = 0\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def get_next_state(self, state, action):\n",
        "    # this answers: where would I end up if I perform action 'action' in state 's'?\n",
        "    i, j = state[0], state[1]\n",
        "\n",
        "    # if this action moves you somewhere else, then it will be in this dictionary\n",
        "    if action == Action.UP:\n",
        "      i -= 1\n",
        "    elif action == Action.DOWN:\n",
        "      i += 1\n",
        "    elif action == Action.RIGHT:\n",
        "      j += 1\n",
        "    elif action == Action.LEFT:\n",
        "      j -= 1\n",
        "\n",
        "    return i, j\n",
        "\n",
        "  def move(self, action):\n",
        "    # check if legal move first\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == Action.UP:\n",
        "        self.i -= 1\n",
        "      elif action == Action.DOWN:\n",
        "        self.i += 1\n",
        "      elif action == Action.RIGHT:\n",
        "        self.j += 1\n",
        "      elif action == Action.LEFT:\n",
        "        self.j -= 1\n",
        "    # return a reward (if any)\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    # these are the opposite of what U/D/L/R should normally do\n",
        "    if action == Action.UP:\n",
        "      self.i += 1\n",
        "    elif action == Action.DOWN:\n",
        "      self.i -= 1\n",
        "    elif action == Action.RIGHT:\n",
        "      self.j -= 1\n",
        "    elif action == Action.LEFT:\n",
        "      self.j += 1\n",
        "    # raise an exception if we arrive somewhere we shouldn't be\n",
        "    # should never happen\n",
        "    assert(self.current_state() in self.all_states())\n",
        "\n",
        "  def game_over(self):\n",
        "    # returns true if game is over, else false\n",
        "    # true if we are in a state where no actions are possible\n",
        "    return (self.i, self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    # possibly buggy but simple way to get all states\n",
        "    # either a position that has possible next actions\n",
        "    # or a position that yields a reward\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())"
      ],
      "metadata": {
        "id": "7fMc7cCCubT9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gridworld():\n",
        "  # define a grid that describes the reward for arriving at each state\n",
        "  # and possible actions at each state\n",
        "  gridworld = Gridworld(ROW_SIZE, COLUMN_SIZE, (ROW_SIZE, 0))\n",
        "\n",
        "  rewards = {}\n",
        "  actions = {}\n",
        "  total_number_of_grids = ROW_SIZE * COLUMN_SIZE\n",
        "  number_of_accessible_grid = int(total_number_of_grids * STATE_PROBS[State.ACCESSIBLE_GRID])\n",
        "  number_of_inaccessible_grid = int(total_number_of_grids * STATE_PROBS[State.INACCESSIBLE_GRID])\n",
        "  # We subtract the number of winner grid which is 1.\n",
        "  number_of_loser_grid = total_number_of_grids - number_of_accessible_grid - number_of_inaccessible_grid - 1\n",
        "\n",
        "  # populate the accessible grid\n",
        "  num_grid = 0\n",
        "  while num_grid < number_of_accessible_grid:\n",
        "    i = np.random.choice(ROW_SIZE)\n",
        "    j = np.random.choice(COLUMN_SIZE)\n",
        "    state = (i, j)\n",
        "    if state not in actions.keys():\n",
        "      num_grid += 1\n",
        "      actions[state] = None\n",
        "\n",
        "  # populate the negative reward grid\n",
        "  num_grid = 0\n",
        "  while num_grid < number_of_loser_grid:\n",
        "    i = np.random.choice(ROW_SIZE)\n",
        "    j = np.random.choice(COLUMN_SIZE)\n",
        "    state = (i, j)\n",
        "    if state not in actions.keys() and state not in rewards.keys():\n",
        "      num_grid += 1\n",
        "      rewards[state] = -10\n",
        "\n",
        "  # populate the positive reward grid\n",
        "  num_grid = 0\n",
        "  while num_grid < 1:\n",
        "    i = np.random.choice(ROW_SIZE)\n",
        "    j = np.random.choice(COLUMN_SIZE)\n",
        "    state = (i, j)\n",
        "    if state not in actions.keys() and state not in rewards.keys():\n",
        "      num_grid += 1\n",
        "      rewards[state] = 10\n",
        "\n",
        "  gridworld.set_rewards(rewards)\n",
        "  gridworld.set_actions(actions)\n",
        "\n",
        "  # populate action space\n",
        "  for key, _ in actions.items():\n",
        "    actions_ = []\n",
        "    for action in ACTION_SPACE:\n",
        "      next_state = gridworld.get_next_state(state=key, action=action)\n",
        "      if next_state in actions.keys() or next_state in rewards.keys():\n",
        "        actions_.append(action)\n",
        "    actions[key] = tuple(actions_)\n",
        "\n",
        "  gridworld.set_actions(actions)\n",
        "  return gridworld"
      ],
      "metadata": {
        "id": "b4RqTitHEJcW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_negative_gridworld(step_cost=-0.1):\n",
        "  # in this game we want to try to minimize the number of moves\n",
        "  # so we will penalize every move\n",
        "  gridworld = create_gridworld()\n",
        "  for key, _ in gridworld.actions:\n",
        "    gridworld.rewards[key] = step_cost\n",
        "  return gridworld"
      ],
      "metadata": {
        "id": "GOaITlboES0Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_values(value_function, gridworld):\n",
        "  for i in range(gridworld.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(gridworld.cols):\n",
        "      value = value_function.get((i,j), 0)\n",
        "      if value >= 0:\n",
        "        print(\" %.2f|\" % value, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % value, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "KC9-4xGmE_Cy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_policy(policy, gridworld):\n",
        "  for i in range(gridworld.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(gridworld.cols):\n",
        "      state = (i, j)\n",
        "      action = policy.get(state, ' ')\n",
        "      print(\"  %s  |\" % action, end=\"\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "sKvENhEyE-4D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transition_probs_and_rewards(gridworld):\n",
        "  ### define transition probabilities and grid ###\n",
        "  # the key is (s, a, s'), the value is the probability\n",
        "  # that is, transition_probs[(s, a, s')] = p(s' | s, a)\n",
        "  # any key NOT present will considered to be impossible (i.e. probability 0)\n",
        "  transition_probs = {}\n",
        "\n",
        "  # to reduce the dimensionality of the dictionary, we'll use deterministic\n",
        "  # rewards, r(s, a, s')\n",
        "  rewards = {}\n",
        "\n",
        "  for i in range(gridworld.rows):\n",
        "    for j in range(gridworld.cols):\n",
        "      state = (i, j)\n",
        "      if not gridworld.is_terminal(state):\n",
        "        for action in ACTION_SPACE:\n",
        "          next_state = gridworld.get_next_state(state, action)\n",
        "          transition_probs[(state, action, next_state)] = 1\n",
        "          if next_state in gridworld.rewards:\n",
        "            rewards[(state, action, next_state)] = gridworld.rewards[next_state]\n",
        "\n",
        "  return transition_probs, rewards"
      ],
      "metadata": {
        "id": "ANn9WB7OE-j8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_deterministic_policy(gridworld, policy, value_function_=None):\n",
        "  # initialize V(s) = 0\n",
        "  if value_function_ is None:\n",
        "    value_function = {}\n",
        "    for state in gridworld.all_states():\n",
        "      value_function[state] = 0\n",
        "  else:\n",
        "    # it's faster to use the existing V(s) since the value won't change\n",
        "    # that much from one policy to the next\n",
        "    value_function = value_function_\n",
        "\n",
        "  # repeat until convergence\n",
        "  it = 0\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for state in gridworld.all_states():\n",
        "      if not gridworld.is_terminal(state):\n",
        "        old_value = value_function[state]\n",
        "        new_value = 0 # we will accumulate the answer\n",
        "        for action in ACTION_SPACE:\n",
        "          for new_state in gridworld.all_states():\n",
        "\n",
        "            # action probability is deterministic\n",
        "            action_prob = 1 if policy.get(state) == action else 0\n",
        "\n",
        "            # reward is a function of (s, a, s'), 0 if not specified\n",
        "            reward = rewards.get((state, action, new_state), 0)\n",
        "            new_value += action_prob * transition_probs.get((state, action, new_state), 0) * (reward + DISCOUNT_FACTOR * value_function[new_state])\n",
        "\n",
        "        # after done getting the new value, update the value table\n",
        "        value_function[state] = new_value\n",
        "        biggest_change = max(biggest_change, np.abs(old_value - new_value))\n",
        "    it += 1\n",
        "    print(f\"Iteration: {it}, Error: {biggest_change}\")\n",
        "    if biggest_change < THRESHOLD:\n",
        "      break\n",
        "  return value_function"
      ],
      "metadata": {
        "id": "f68Yv8xOFIkV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  gridworld = create_gridworld()\n",
        "  transition_probs, rewards = get_transition_probs_and_rewards(gridworld)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(gridworld.rewards, gridworld)\n",
        "  print()\n",
        "\n",
        "  # state -> action\n",
        "  # we'll randomly choose an action and update as we learn\n",
        "  policy = {}\n",
        "  for state in gridworld.actions.keys():\n",
        "    policy[state] = np.random.choice(ACTION_SPACE)\n",
        "\n",
        "  # initial policy\n",
        "  print(\"initial policy:\")\n",
        "  print_policy(policy, gridworld)\n",
        "  print()\n",
        "\n",
        "  value_function = None\n",
        "\n",
        "  # policy evaluation step - we already know how to do this!\n",
        "  value_function = evaluate_deterministic_policy(gridworld, policy, value_function_=value_function)\n",
        "\n",
        "  # once we're done, print the final policy and values\n",
        "  print(\"values:\")\n",
        "  print_values(value_function, gridworld)\n",
        "  print()\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, gridworld)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwhrZQCUnqr-",
        "outputId": "1b3191c1-a717-4c90-9b17-cebbdbd73f47"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 10.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|-10.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00|-10.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|-10.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|-10.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "initial policy:\n",
            "---------------------------\n",
            "     |  D  |     |     |  U  |     |  R  |  L  |     |     |\n",
            "---------------------------\n",
            "  L  |  U  |     |  R  |  D  |  U  |  D  |  D  |  U  |  R  |\n",
            "---------------------------\n",
            "  U  |     |  U  |  R  |  L  |  U  |  R  |  R  |     |  D  |\n",
            "---------------------------\n",
            "  L  |     |     |     |  R  |     |  L  |  R  |  U  |     |\n",
            "---------------------------\n",
            "  D  |  L  |     |     |  U  |     |     |  U  |  D  |  L  |\n",
            "---------------------------\n",
            "  L  |  D  |     |  L  |     |     |  D  |     |  R  |  R  |\n",
            "---------------------------\n",
            "  L  |  L  |     |  D  |  L  |     |     |     |  R  |  R  |\n",
            "---------------------------\n",
            "     |     |     |     |  R  |     |  D  |     |     |     |\n",
            "---------------------------\n",
            "  L  |     |  L  |     |     |  R  |  D  |  D  |  L  |  U  |\n",
            "---------------------------\n",
            "  D  |     |  D  |  L  |  L  |     |  D  |  D  |  U  |     |\n",
            "\n",
            "Iteration: 1, Error: 0\n",
            "values:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "policy:\n",
            "---------------------------\n",
            "     |  D  |     |     |  U  |     |  R  |  L  |     |     |\n",
            "---------------------------\n",
            "  L  |  U  |     |  R  |  D  |  U  |  D  |  D  |  U  |  R  |\n",
            "---------------------------\n",
            "  U  |     |  U  |  R  |  L  |  U  |  R  |  R  |     |  D  |\n",
            "---------------------------\n",
            "  L  |     |     |     |  R  |     |  L  |  R  |  U  |     |\n",
            "---------------------------\n",
            "  D  |  L  |     |     |  U  |     |     |  U  |  D  |  L  |\n",
            "---------------------------\n",
            "  L  |  D  |     |  L  |     |     |  D  |     |  R  |  R  |\n",
            "---------------------------\n",
            "  L  |  L  |     |  D  |  L  |     |     |     |  R  |  R  |\n",
            "---------------------------\n",
            "     |     |     |     |  R  |     |  D  |     |     |     |\n",
            "---------------------------\n",
            "  L  |     |  L  |     |     |  R  |  D  |  D  |  L  |  U  |\n",
            "---------------------------\n",
            "  D  |     |  D  |  L  |  L  |     |  D  |  D  |  U  |     |\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UObQkVjEn-OH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}