{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from enum import IntEnum, Enum"
      ],
      "metadata": {
        "id": "A2BpANMrznNx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Action(Enum):\n",
        "  UP = 'U'\n",
        "  DOWN = 'D'\n",
        "  LEFT = 'L'\n",
        "  RIGHT = 'R'\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.value)"
      ],
      "metadata": {
        "id": "oaOGC8bt14LN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(IntEnum):\n",
        "  ACCESSIBLE_GRID = 0\n",
        "  INACCESSIBLE_GRID = -2\n",
        "  LOSER_GRID = -1\n",
        "  WINNER_GRID = 1"
      ],
      "metadata": {
        "id": "HovS_zDS5LvC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION_SPACE = (Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT)\n",
        "STATE_PROBS = [0.6, 0.35, 0.05] # prob of accessible grid, prob of inaccessible grid, prob of loser grid\n",
        "STATES = [State.ACCESSIBLE_GRID, State.INACCESSIBLE_GRID, State.LOSER_GRID, State.WINNER_GRID]\n",
        "UNKNOWN_POLICY = -2 # the policy is unknown for now, the policies are going to be determined after creating the gridworld\n",
        "ROW_SIZE = 10\n",
        "COLUMN_SIZE = 10\n",
        "THRESHOLD = 1e-3\n",
        "DISCOUNT_FACTOR = 0.9"
      ],
      "metadata": {
        "id": "i3egGS_kzpis"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindyGridworld: # Environment\n",
        "  def __init__(self, rows, cols, start):\n",
        "    self.rows = rows\n",
        "    self.cols = cols\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions, probs):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "    self.probs = probs\n",
        "\n",
        "  def set_rewards(self, rewards):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    self.rewards = rewards\n",
        "\n",
        "  def set_actions(self, actions):\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_probs(self, probs):\n",
        "    self.probs = probs\n",
        "\n",
        "  def set_state(self, s):\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def is_terminal(self, state):\n",
        "    return state not in self.actions\n",
        "\n",
        "  def reset(self):\n",
        "    # put agent back in start position\n",
        "    self.i = ROW_SIZE - 1\n",
        "    self.j = 0\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def get_next_state(self, state, action):\n",
        "    # this answers: where would I end up if I perform action 'action' in state 's'?\n",
        "    i, j = state[0], state[1]\n",
        "\n",
        "    # if this action moves you somewhere else, then it will be in this dictionary\n",
        "    if action == Action.UP:\n",
        "      i -= 1\n",
        "    elif action == Action.DOWN:\n",
        "      i += 1\n",
        "    elif action == Action.RIGHT:\n",
        "      j += 1\n",
        "    elif action == Action.LEFT:\n",
        "      j -= 1\n",
        "\n",
        "    return i, j\n",
        "\n",
        "  def move(self, action):\n",
        "    state = (self.i, self.j)\n",
        "\n",
        "    next_state_probs = self.probs[(state, action)]\n",
        "    next_states = list(self.probs.keys())\n",
        "    next_probs = list(self.probs.values())\n",
        "    next_state_idx = np.random.choice(len(next_states), p=next_probs)\n",
        "\n",
        "    next_state = next_states[next_state_idx]\n",
        "\n",
        "    # update the current state\n",
        "    self.i, self.j = next_state\n",
        "\n",
        "    # return a reward (if any)\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def game_over(self):\n",
        "    # returns true if game is over, else false\n",
        "    # true if we are in a state where no actions are possible\n",
        "    return (self.i, self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    # possibly buggy but simple way to get all states\n",
        "    # either a position that has possible next actions\n",
        "    # or a position that yields a reward\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())"
      ],
      "metadata": {
        "id": "7fMc7cCCubT9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_windy_gridworld():\n",
        "  # define a grid that describes the reward for arriving at each state\n",
        "  # and possible actions at each state\n",
        "  gridworld = WindyGridworld(ROW_SIZE, COLUMN_SIZE, (ROW_SIZE, 0))\n",
        "\n",
        "  rewards = {}\n",
        "  actions = {}\n",
        "  probs = {}\n",
        "  total_number_of_grids = ROW_SIZE * COLUMN_SIZE\n",
        "  number_of_accessible_grid = int(total_number_of_grids * STATE_PROBS[State.ACCESSIBLE_GRID])\n",
        "  number_of_inaccessible_grid = int(total_number_of_grids * STATE_PROBS[State.INACCESSIBLE_GRID])\n",
        "  # We subtract the number of winner grid which is 1.\n",
        "  number_of_loser_grid = total_number_of_grids - number_of_accessible_grid - number_of_inaccessible_grid - 1\n",
        "\n",
        "  # populate the accessible grid\n",
        "  num_grid = 0\n",
        "  while num_grid < number_of_accessible_grid:\n",
        "    i = np.random.choice(ROW_SIZE)\n",
        "    j = np.random.choice(COLUMN_SIZE)\n",
        "    state = (i, j)\n",
        "    if state not in actions.keys():\n",
        "      num_grid += 1\n",
        "      actions[state] = None\n",
        "\n",
        "  # populate the negative reward grid\n",
        "  num_grid = 0\n",
        "  while num_grid < number_of_loser_grid:\n",
        "    i = np.random.choice(ROW_SIZE)\n",
        "    j = np.random.choice(COLUMN_SIZE)\n",
        "    state = (i, j)\n",
        "    if state not in actions.keys() and state not in rewards.keys():\n",
        "      num_grid += 1\n",
        "      rewards[state] = -10\n",
        "\n",
        "  # populate the positive reward grid\n",
        "  num_grid = 0\n",
        "  while num_grid < 1:\n",
        "    i = np.random.choice(ROW_SIZE)\n",
        "    j = np.random.choice(COLUMN_SIZE)\n",
        "    state = (i, j)\n",
        "    if state not in actions.keys() and state not in rewards.keys():\n",
        "      num_grid += 1\n",
        "      rewards[state] = 10\n",
        "\n",
        "  gridworld.set_rewards(rewards)\n",
        "  gridworld.set_actions(actions)\n",
        "\n",
        "  # populate action space\n",
        "  for key, _ in actions.items():\n",
        "    actions_ = []\n",
        "    for action in ACTION_SPACE:\n",
        "      next_state = gridworld.get_next_state(state=key, action=action)\n",
        "      if next_state in actions.keys() or next_state in rewards.keys():\n",
        "        actions_.append(action)\n",
        "    actions[key] = tuple(actions_)\n",
        "\n",
        "  gridworld.set_actions(actions)\n",
        "\n",
        "  # populate prob space\n",
        "  for key, _ in actions.items():\n",
        "    for action in ACTION_SPACE:\n",
        "      next_state = gridworld.get_next_state(state=key, action=action)\n",
        "      if next_state in actions.keys() or next_state in rewards.keys():\n",
        "        probs[(key, action, next_state)] = 1 / len(_)\n",
        "\n",
        "  gridworld.set_probs(probs)\n",
        "  return gridworld"
      ],
      "metadata": {
        "id": "b4RqTitHEJcW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_negative_windy_gridworld(step_cost=-0.1):\n",
        "  # in this game we want to try to minimize the number of moves\n",
        "  # so we will penalize every move\n",
        "  gridworld = create_windy_gridworld()\n",
        "  for key, _ in gridworld.actions:\n",
        "    gridworld.rewards[key] = step_cost\n",
        "  return gridworld"
      ],
      "metadata": {
        "id": "GOaITlboES0Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_values(value_function, gridworld):\n",
        "  for i in range(gridworld.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(gridworld.cols):\n",
        "      value = value_function.get((i,j), 0)\n",
        "      if value >= 0:\n",
        "        print(\" %.2f|\" % value, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % value, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "KC9-4xGmE_Cy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_policy(policy, gridworld):\n",
        "  for i in range(gridworld.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(gridworld.cols):\n",
        "      state = (i, j)\n",
        "      action = policy.get(state, ' ')\n",
        "      print(\"  %s  |\" % action, end=\"\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "sKvENhEyE-4D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transition_probs_and_rewards(gridworld):\n",
        "  ### define transition probabilities and grid ###\n",
        "  # the key is (s, a, s'), the value is the probability\n",
        "  # that is, transition_probs[(s, a, s')] = p(s' | s, a)\n",
        "  # any key NOT present will considered to be impossible (i.e. probability 0)\n",
        "  transition_probs = {}\n",
        "\n",
        "  # to reduce the dimensionality of the dictionary, we'll use deterministic\n",
        "  # rewards, r(s, a, s')\n",
        "  rewards = {}\n",
        "\n",
        "  for (state, action, next_state), prob in gridworld.probs.items():\n",
        "      transition_probs[(state, action, next_state)] = prob\n",
        "      rewards[(state, action, next_state)] = gridworld.rewards.get(next_state, 0)\n",
        "\n",
        "  return transition_probs, rewards"
      ],
      "metadata": {
        "id": "ANn9WB7OE-j8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_deterministic_policy(gridworld, policy, value_function_=None):\n",
        "  # initialize V(s) = 0\n",
        "  if value_function_ is None:\n",
        "    value_function = {}\n",
        "    for state in gridworld.all_states():\n",
        "      value_function[state] = 0\n",
        "  else:\n",
        "    # it's faster to use the existing V(s) since the value won't change\n",
        "    # that much from one policy to the next\n",
        "    value_function = value_function_\n",
        "\n",
        "  # repeat until convergence\n",
        "  it = 0\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for state in gridworld.all_states():\n",
        "      if not gridworld.is_terminal(state):\n",
        "        old_value = value_function[state]\n",
        "        new_value = 0 # we will accumulate the answer\n",
        "        for action in ACTION_SPACE:\n",
        "          for new_state in gridworld.all_states():\n",
        "\n",
        "            # action probability is deterministic\n",
        "            action_prob = 1 if policy.get(state) == action else 0\n",
        "\n",
        "            # reward is a function of (s, a, s'), 0 if not specified\n",
        "            reward = rewards.get((state, action, new_state), 0)\n",
        "            new_value += action_prob * transition_probs.get((state, action, new_state), 0) * (reward + DISCOUNT_FACTOR * value_function[new_state])\n",
        "\n",
        "        # after done getting the new value, update the value table\n",
        "        value_function[state] = new_value\n",
        "        biggest_change = max(biggest_change, np.abs(old_value - new_value))\n",
        "    it += 1\n",
        "    print(f\"Iteration: {it}, Error: {biggest_change}\")\n",
        "    if biggest_change < THRESHOLD:\n",
        "      break\n",
        "  return value_function"
      ],
      "metadata": {
        "id": "f68Yv8xOFIkV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  windy_gridworld = create_negative_windy_gridworld(-0.1)\n",
        "  transition_probs, rewards = get_transition_probs_and_rewards(windy_gridworld)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(windy_gridworld.rewards, windy_gridworld)\n",
        "  print()\n",
        "\n",
        "  # state -> action\n",
        "  # we'll randomly choose an action and update as we learn\n",
        "  policy = {}\n",
        "  for state in windy_gridworld.actions.keys():\n",
        "    policy[state] = np.random.choice(ACTION_SPACE)\n",
        "\n",
        "  # initial policy\n",
        "  print(\"initial policy:\")\n",
        "  print_policy(policy, windy_gridworld)\n",
        "  print()\n",
        "\n",
        "  # repeat until convergence - will break out when policy does not change\n",
        "  value_function = None\n",
        "  while True:\n",
        "\n",
        "    # policy evaluation step - we already know how to do this!\n",
        "    value_function = evaluate_deterministic_policy(windy_gridworld, policy, value_function_=value_function)\n",
        "\n",
        "    # policy improvement step\n",
        "    is_policy_converged = True\n",
        "    for state in windy_gridworld.actions.keys():\n",
        "      old_action = policy[state]\n",
        "      new_action = None\n",
        "      best_value = float('-inf')\n",
        "\n",
        "      # loop through all possible actions to find the best current action\n",
        "      for action in ACTION_SPACE:\n",
        "        value = 0\n",
        "        for new_state in windy_gridworld.all_states():\n",
        "          # reward is a function of (s, a, s'), 0 if not specified\n",
        "          reward = rewards.get((state, action, new_state), 0)\n",
        "          value += transition_probs.get((state, action, new_state), 0) * (reward + DISCOUNT_FACTOR * value_function[new_state])\n",
        "\n",
        "        if value > best_value:\n",
        "          best_value = value\n",
        "          new_action = action\n",
        "\n",
        "      # new_a now represents the best action in this state\n",
        "      policy[state] = new_action\n",
        "      if new_action != old_action:\n",
        "        is_policy_converged = False\n",
        "\n",
        "    if is_policy_converged:\n",
        "      break\n",
        "\n",
        "  # once we're done, print the final policy and values\n",
        "  print(\"values:\")\n",
        "  print_values(value_function, windy_gridworld)\n",
        "  print()\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, windy_gridworld)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhZXEhZYufUK",
        "outputId": "b36baa74-00d1-48c2-811d-a031a114d982"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|-10.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            "-10.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00|-10.00| 0.00| 0.00| 0.00| 0.00| 10.00| 0.00| 0.00|-10.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "initial policy:\n",
            "---------------------------\n",
            "  L  |     |  U  |  U  |  L  |     |  L  |     |  L  |  D  |\n",
            "---------------------------\n",
            "  R  |     |  U  |     |     |  D  |  R  |     |     |  U  |\n",
            "---------------------------\n",
            "     |  R  |  L  |     |  L  |     |  L  |     |  D  |  L  |\n",
            "---------------------------\n",
            "     |     |     |     |  U  |     |  L  |  L  |  L  |  U  |\n",
            "---------------------------\n",
            "     |     |  L  |  D  |  L  |     |     |  R  |     |  L  |\n",
            "---------------------------\n",
            "     |     |  U  |     |     |  U  |  D  |     |  U  |  R  |\n",
            "---------------------------\n",
            "     |  R  |  L  |  R  |  D  |  R  |  D  |  R  |  U  |  R  |\n",
            "---------------------------\n",
            "  L  |     |     |     |  D  |  L  |     |  L  |  D  |     |\n",
            "---------------------------\n",
            "  D  |  L  |     |  L  |  L  |  L  |     |  D  |  L  |  U  |\n",
            "---------------------------\n",
            "  L  |  L  |     |  L  |     |  D  |     |  L  |     |     |\n",
            "\n",
            "Iteration: 1, Error: 5.0\n",
            "Iteration: 2, Error: 1.125\n",
            "Iteration: 3, Error: 0\n",
            "Iteration: 1, Error: 5.0\n",
            "Iteration: 2, Error: 0.75\n",
            "Iteration: 3, Error: 0.675\n",
            "Iteration: 4, Error: 0\n",
            "Iteration: 1, Error: 0.699375\n",
            "Iteration: 2, Error: 0.094415625\n",
            "Iteration: 3, Error: 0\n",
            "Iteration: 1, Error: 0.0759375\n",
            "Iteration: 2, Error: 0.007593750000000007\n",
            "Iteration: 3, Error: 0\n",
            "Iteration: 1, Error: 0.022781250000000003\n",
            "Iteration: 2, Error: 0\n",
            "Iteration: 1, Error: 0.010251562500000002\n",
            "Iteration: 2, Error: 0\n",
            "Iteration: 1, Error: 0.003075468750000001\n",
            "Iteration: 2, Error: 0\n",
            "Iteration: 1, Error: 0.0012455648437500002\n",
            "Iteration: 2, Error: 0\n",
            "Iteration: 1, Error: 0.0005605041796875001\n",
            "Iteration: 1, Error: 0.00018683472656250009\n",
            "Iteration: 1, Error: 0.0001681512539062501\n",
            "Iteration: 1, Error: 1.6815125390625008e-05\n",
            "Iteration: 1, Error: 1.5133612851562508e-05\n",
            "values:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.01| 0.00| 0.00| 0.00| 0.00| 0.00| 0.00| 0.01|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.01| 0.00| 0.00| 0.51| 1.12| 0.00| 0.08| 0.02|\n",
            "---------------------------\n",
            " 0.00| 0.01| 0.03| 0.10| 0.23| 0.56| 2.50| 0.75| 0.17| 0.05|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00| 0.75| 2.50| 0.00| 2.50| 0.56| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.10| 0.23| 0.75| 0.00| 0.75| 0.23| 0.10|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.09| 0.00| 0.68| 0.00| 0.68| 0.00| 0.00|\n",
            "\n",
            "policy:\n",
            "---------------------------\n",
            "  U  |     |  U  |  U  |  U  |     |  D  |     |  R  |  D  |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |     |  R  |  D  |     |     |  D  |\n",
            "---------------------------\n",
            "     |  U  |  U  |     |  D  |     |  D  |     |  D  |  D  |\n",
            "---------------------------\n",
            "     |     |     |     |  D  |     |  R  |  R  |  R  |  D  |\n",
            "---------------------------\n",
            "     |     |  D  |  L  |  L  |     |     |  U  |     |  D  |\n",
            "---------------------------\n",
            "     |     |  D  |     |     |  R  |  D  |     |  D  |  L  |\n",
            "---------------------------\n",
            "     |  R  |  R  |  R  |  D  |  D  |  D  |  D  |  L  |  L  |\n",
            "---------------------------\n",
            "  D  |     |     |     |  R  |  R  |     |  L  |  L  |     |\n",
            "---------------------------\n",
            "  U  |  D  |     |  R  |  U  |  U  |     |  U  |  L  |  L  |\n",
            "---------------------------\n",
            "  U  |  U  |     |  U  |     |  U  |     |  U  |     |     |\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tO2JlkySvnqz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}