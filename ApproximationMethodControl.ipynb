{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A2BpANMrznNx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from enum import IntEnum, Enum\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.kernel_approximation import Nystroem, RBFSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oaOGC8bt14LN"
      },
      "outputs": [],
      "source": [
        "class Action(Enum):\n",
        "  UP = 'U'\n",
        "  DOWN = 'D'\n",
        "  LEFT = 'L'\n",
        "  RIGHT = 'R'\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.value)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionInt(IntEnum):\n",
        "  UP = 0\n",
        "  DOWN = 1\n",
        "  LEFT = 2\n",
        "  RIGHT = 3"
      ],
      "metadata": {
        "id": "l5Um-qUVCr35"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HovS_zDS5LvC"
      },
      "outputs": [],
      "source": [
        "class State(IntEnum):\n",
        "  ACCESSIBLE_GRID = 0\n",
        "  INACCESSIBLE_GRID = -2\n",
        "  LOSER_GRID = -1\n",
        "  WINNER_GRID = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i3egGS_kzpis"
      },
      "outputs": [],
      "source": [
        "ACTION_SPACE = (Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT)\n",
        "ACTION_SPACE_INT = (ActionInt.UP, ActionInt.DOWN, ActionInt.LEFT, ActionInt.RIGHT)\n",
        "ACTION_SPACE_ONE_HOT = ((1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1))\n",
        "STATE_PROBS = [0.6, 0.35, 0.05] # prob of accessible grid, prob of inaccessible grid, prob of loser grid\n",
        "STATES = [State.ACCESSIBLE_GRID, State.INACCESSIBLE_GRID, State.LOSER_GRID, State.WINNER_GRID]\n",
        "UNKNOWN_POLICY = -2 # the policy is unknown for now, the policies are going to be determined after creating the gridworld\n",
        "ROW_SIZE = 10\n",
        "COLUMN_SIZE = 10\n",
        "THRESHOLD = 1e-3\n",
        "DISCOUNT_FACTOR = 0.9\n",
        "LR = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7fMc7cCCubT9"
      },
      "outputs": [],
      "source": [
        "class Gridworld: # Environment\n",
        "  def __init__(self, rows, cols, start):\n",
        "    self.rows = rows\n",
        "    self.cols = cols\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_rewards(self, rewards):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    self.rewards = rewards\n",
        "\n",
        "  def set_actions(self, actions):\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self, s):\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def is_terminal(self, state):\n",
        "    return state not in self.actions\n",
        "\n",
        "  def reset(self):\n",
        "    # put agent back in start position\n",
        "    self.i = ROW_SIZE - 1\n",
        "    self.j = 0\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def get_next_state(self, state, action):\n",
        "    # this answers: where would I end up if I perform action 'action' in state 's'?\n",
        "    i, j = state[0], state[1]\n",
        "\n",
        "    # if this action moves you somewhere else, then it will be in this dictionary\n",
        "    if action == Action.UP:\n",
        "      i -= 1\n",
        "    elif action == Action.DOWN:\n",
        "      i += 1\n",
        "    elif action == Action.RIGHT:\n",
        "      j += 1\n",
        "    elif action == Action.LEFT:\n",
        "      j -= 1\n",
        "\n",
        "    return i, j\n",
        "\n",
        "  def move(self, action):\n",
        "    # check if legal move first\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == Action.UP:\n",
        "        self.i -= 1\n",
        "      elif action == Action.DOWN:\n",
        "        self.i += 1\n",
        "      elif action == Action.RIGHT:\n",
        "        self.j += 1\n",
        "      elif action == Action.LEFT:\n",
        "        self.j -= 1\n",
        "    # return a reward (if any)\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    # these are the opposite of what U/D/L/R should normally do\n",
        "    if action == Action.UP:\n",
        "      self.i += 1\n",
        "    elif action == Action.DOWN:\n",
        "      self.i -= 1\n",
        "    elif action == Action.RIGHT:\n",
        "      self.j -= 1\n",
        "    elif action == Action.LEFT:\n",
        "      self.j += 1\n",
        "    # raise an exception if we arrive somewhere we shouldn't be\n",
        "    # should never happen\n",
        "    assert(self.current_state() in self.all_states())\n",
        "\n",
        "  def game_over(self):\n",
        "    # returns true if game is over, else false\n",
        "    # true if we are in a state where no actions are possible\n",
        "    return (self.i, self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    # possibly buggy but simple way to get all states\n",
        "    # either a position that has possible next actions\n",
        "    # or a position that yields a reward\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b4RqTitHEJcW"
      },
      "outputs": [],
      "source": [
        "def create_gridworld():\n",
        "  # define a grid that describes the reward for arriving at each state\n",
        "  # and possible actions at each state\n",
        "  gridworld = Gridworld(ROW_SIZE, COLUMN_SIZE, (ROW_SIZE, 0))\n",
        "\n",
        "  rewards = {}\n",
        "  actions = {}\n",
        "  total_number_of_grids = ROW_SIZE * COLUMN_SIZE\n",
        "  number_of_accessible_grid = int(total_number_of_grids * STATE_PROBS[State.ACCESSIBLE_GRID])\n",
        "  number_of_inaccessible_grid = int(total_number_of_grids * STATE_PROBS[State.INACCESSIBLE_GRID])\n",
        "  # We subtract the number of winner grid which is 1.\n",
        "  number_of_loser_grid = total_number_of_grids - number_of_accessible_grid - number_of_inaccessible_grid - 1\n",
        "\n",
        "  # populate the accessible grid\n",
        "  num_grid = 0\n",
        "  while num_grid < number_of_accessible_grid:\n",
        "    i = np.random.choice(ROW_SIZE)\n",
        "    j = np.random.choice(COLUMN_SIZE)\n",
        "    state = (i, j)\n",
        "    if state not in actions.keys():\n",
        "      num_grid += 1\n",
        "      actions[state] = None\n",
        "\n",
        "  # populate the negative reward grid\n",
        "  num_grid = 0\n",
        "  while num_grid < number_of_loser_grid:\n",
        "    i = np.random.choice(ROW_SIZE)\n",
        "    j = np.random.choice(COLUMN_SIZE)\n",
        "    state = (i, j)\n",
        "    if state not in actions.keys() and state not in rewards.keys():\n",
        "      num_grid += 1\n",
        "      rewards[state] = -10\n",
        "\n",
        "  # populate the positive reward grid\n",
        "  num_grid = 0\n",
        "  while num_grid < 1:\n",
        "    i = np.random.choice(ROW_SIZE)\n",
        "    j = np.random.choice(COLUMN_SIZE)\n",
        "    state = (i, j)\n",
        "    if state not in actions.keys() and state not in rewards.keys():\n",
        "      num_grid += 1\n",
        "      rewards[state] = 10\n",
        "\n",
        "  gridworld.set_rewards(rewards)\n",
        "  gridworld.set_actions(actions)\n",
        "\n",
        "  # populate action space\n",
        "  for key, _ in actions.items():\n",
        "    actions_ = []\n",
        "    for action in ACTION_SPACE:\n",
        "      next_state = gridworld.get_next_state(state=key, action=action)\n",
        "      if next_state in actions.keys() or next_state in rewards.keys():\n",
        "        actions_.append(action)\n",
        "    actions[key] = tuple(actions_)\n",
        "\n",
        "  gridworld.set_actions(actions)\n",
        "  return gridworld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GOaITlboES0Y"
      },
      "outputs": [],
      "source": [
        "def create_negative_gridworld(step_cost=-0.1):\n",
        "  # in this game we want to try to minimize the number of moves\n",
        "  # so we will penalize every move\n",
        "  gridworld = create_gridworld()\n",
        "  for key, _ in gridworld.actions:\n",
        "    gridworld.rewards[key] = step_cost\n",
        "  return gridworld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KC9-4xGmE_Cy"
      },
      "outputs": [],
      "source": [
        "def print_values(value_function, gridworld):\n",
        "  for i in range(gridworld.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(gridworld.cols):\n",
        "      value = value_function.get((i,j), 0)\n",
        "      if value >= 0:\n",
        "        print(\" %.2f|\" % value, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % value, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sKvENhEyE-4D"
      },
      "outputs": [],
      "source": [
        "def print_policy(policy, gridworld):\n",
        "  for i in range(gridworld.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(gridworld.cols):\n",
        "      state = (i, j)\n",
        "      action = policy.get(state, ' ')\n",
        "      print(\"  %s  |\" % action, end=\"\")\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I7QDMdo0PD0M"
      },
      "outputs": [],
      "source": [
        "def play_game(gridworld, policy, max_steps=20):\n",
        "  start_states = list(gridworld.actions.keys())\n",
        "  start_state = start_states[np.random.choice(len(start_states))]\n",
        "  gridworld.set_state(start_state)\n",
        "\n",
        "  state = gridworld.current_state()\n",
        "  states = [state]\n",
        "  rewards = [0]\n",
        "\n",
        "  steps = 0\n",
        "  while not gridworld.game_over() and steps < max_steps:\n",
        "    action = policy[state]\n",
        "    reward = gridworld.move(action)\n",
        "    next_state = gridworld.current_state()\n",
        "\n",
        "    # update state and reward list\n",
        "    states.append(next_state)\n",
        "    rewards.append(reward)\n",
        "    steps += 1\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "  return states, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ANn9WB7OE-j8"
      },
      "outputs": [],
      "source": [
        "def get_transition_probs_and_rewards(gridworld):\n",
        "  ### define transition probabilities and grid ###\n",
        "  # the key is (s, a, s'), the value is the probability\n",
        "  # that is, transition_probs[(s, a, s')] = p(s' | s, a)\n",
        "  # any key NOT present will considered to be impossible (i.e. probability 0)\n",
        "  transition_probs = {}\n",
        "\n",
        "  # to reduce the dimensionality of the dictionary, we'll use deterministic\n",
        "  # rewards, r(s, a, s')\n",
        "  rewards = {}\n",
        "\n",
        "  for i in range(gridworld.rows):\n",
        "    for j in range(gridworld.cols):\n",
        "      state = (i, j)\n",
        "      if not gridworld.is_terminal(state):\n",
        "        for action in ACTION_SPACE:\n",
        "          next_state = gridworld.get_next_state(state, action)\n",
        "          transition_probs[(state, action, next_state)] = 1\n",
        "          if next_state in gridworld.rewards:\n",
        "            rewards[(state, action, next_state)] = gridworld.rewards[next_state]\n",
        "\n",
        "  return transition_probs, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f68Yv8xOFIkV"
      },
      "outputs": [],
      "source": [
        "def evaluate_deterministic_policy(gridworld, policy, value_function_=None):\n",
        "  # initialize V(s) = 0\n",
        "  if value_function_ is None:\n",
        "    value_function = {}\n",
        "    for state in gridworld.all_states():\n",
        "      value_function[state] = 0\n",
        "  else:\n",
        "    # it's faster to use the existing V(s) since the value won't change\n",
        "    # that much from one policy to the next\n",
        "    value_function = value_function_\n",
        "\n",
        "  # repeat until convergence\n",
        "  it = 0\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for state in gridworld.all_states():\n",
        "      if not gridworld.is_terminal(state):\n",
        "        old_value = value_function[state]\n",
        "        new_value = 0 # we will accumulate the answer\n",
        "        for action in ACTION_SPACE:\n",
        "          for new_state in gridworld.all_states():\n",
        "\n",
        "            # action probability is deterministic\n",
        "            action_prob = 1 if policy.get(state) == action else 0\n",
        "\n",
        "            # reward is a function of (s, a, s'), 0 if not specified\n",
        "            reward = rewards.get((state, action, new_state), 0)\n",
        "            new_value += action_prob * transition_probs.get((state, action, new_state), 0) * (reward + DISCOUNT_FACTOR * value_function[new_state])\n",
        "\n",
        "        # after done getting the new value, update the value table\n",
        "        value_function[state] = new_value\n",
        "        biggest_change = max(biggest_change, np.abs(old_value - new_value))\n",
        "    it += 1\n",
        "    print(f\"Iteration: {it}, Error: {biggest_change}\")\n",
        "    if biggest_change < THRESHOLD:\n",
        "      break\n",
        "  return value_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ViUawkY5wsU8"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(model, state, epsilon=0.1):\n",
        "  prob = np.random.random()\n",
        "  if prob < 1 - epsilon:\n",
        "    values = model.predict_all_actions()\n",
        "    return ACTION_SPACE[np.argmax(values)]\n",
        "  else:\n",
        "    return np.random.choice(ACTION_SPACE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(action):\n",
        "  return ACTION_SPACE_ONE_HOT[action]"
      ],
      "metadata": {
        "id": "8EhuUlEjDmBx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def state_action_vector(state, action):\n",
        "  return np.concatenate((state, one_hot(action)))"
      ],
      "metadata": {
        "id": "QbPses6TDwUa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dc-EoNLdxA1B"
      },
      "outputs": [],
      "source": [
        "def gather_samples(gridworld, n_episodes=40):\n",
        "  samples = []\n",
        "  for i in range(n_episodes):\n",
        "    state = gridworld.reset()\n",
        "    while not gridworld.game_over():\n",
        "      action = np.random.choice(ACTION_SPACE_INT)\n",
        "      samples.append(state_action_vector(state, action))\n",
        "      reward = gridworld.move(action)\n",
        "      state = gridworld.current_state()\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sry-RfH2xuGT"
      },
      "outputs": [],
      "source": [
        "class RLModel:\n",
        "  def __init__(self, gridworld):\n",
        "    samples = gather_samples(gridworld)\n",
        "    # self.featurizer = Nystroem()\n",
        "    self.featurizer = RBFSampler()\n",
        "    self.featurizer.fit(samples.reshape(1, -1))\n",
        "    dims = self.featurizer.n_components\n",
        "    # initialize linear model weights\n",
        "    self.w = np.zeros(dims)\n",
        "\n",
        "  def predict(self, state, action):\n",
        "    state_action_vec = state_action_vector(state, action)\n",
        "    x = self.featurizer.transform([state_action_vec])[0]\n",
        "    return x @ self.w\n",
        "\n",
        "  def predict_all_actions(self, state):\n",
        "    return [self.predict(state, action) for action in ACTION_SPACE_INT]\n",
        "\n",
        "  def grad(self, state, action):\n",
        "    state_action_vec = state_action_vector(state, action)\n",
        "    x = self.featurizer.transform([state_action_vec])[0]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhZXEhZYufUK"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  gridworld = create_negative_gridworld(-0.1)\n",
        "\n",
        "  model = RLModel(gridworld)\n",
        "  reward_per_episode = []\n",
        "  state_visit_count = {}\n",
        "\n",
        "  n_episodes = 20000\n",
        "\n",
        "  for it in range(n_episodes):\n",
        "    if it % 2000 == 0:\n",
        "      print(f\"Iteration: {it}\")\n",
        "\n",
        "    state = gridworld.reset()\n",
        "    state_visit_count[state] = state_visit_count.get(state, 0) + 1\n",
        "    episode_reward = 0\n",
        "\n",
        "    while not gridworld.game_over():\n",
        "      action = epsilon_greedy(model, state)\n",
        "      reward = gridworld.move(action)\n",
        "      next_state = gridworld.current_state()\n",
        "      state_visit_count[next_state] = state_visit_count.get(next_state, 0) + 1\n",
        "\n",
        "      if gridworld.is_terminal(next_state):\n",
        "        target = reward\n",
        "      else:\n",
        "        value_next_state = model.predict_all_actions(next_state)\n",
        "        target = reward + DISCOUNT_FACTOR * np.max(value_next_state)\n",
        "\n",
        "      gradient = model.grad(state)\n",
        "      error = target - model.predict(state, action)\n",
        "      model.w += LR * error * gradient\n",
        "\n",
        "      # accumulate error\n",
        "      episode_reward += reward\n",
        "\n",
        "      # update state\n",
        "      state = next_state\n",
        "\n",
        "    reward_per_episode.append(episode_reward)\n",
        "\n",
        "  plt.plot(reward_per_episode)\n",
        "  plt.title(\"Reward per Episode\")\n",
        "  plt.show();\n",
        "\n",
        "  # obtain predicted values\n",
        "  value_function = {}\n",
        "  greedy_policy = {}\n",
        "  states = gridworld.all_states()\n",
        "  for state in states:\n",
        "    if gridworld.is_terminal(state):\n",
        "      value_function[state] = 0\n",
        "    else:\n",
        "      values = model.predict_all_actions(state)\n",
        "      value_function_state = np.max(values)\n",
        "      greedy_policy[state] = ACTION_SPACE[np.argmax(values)]\n",
        "\n",
        "  # once we're done, print the final policy and values\n",
        "  print(\"values:\")\n",
        "  print_values(value_function, gridworld)\n",
        "  print()\n",
        "  print(\"policy:\")\n",
        "  print_policy(greedy_policy, gridworld)\n",
        "  print()\n",
        "\n",
        "  print(\"state_visit_count:\")\n",
        "  state_sample_count_arr = np.zeros((gridworld.rows, gridworld.cols))\n",
        "  for i in range(gridworld.rows):\n",
        "    for j in range(gridworld.cols):\n",
        "      if (i, j) in state_visit_count:\n",
        "        state_sample_count_arr[i,j] = state_visit_count[(i, j)]\n",
        "  df = pd.DataFrame(state_sample_count_arr)\n",
        "  print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6pTppef3MQP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}